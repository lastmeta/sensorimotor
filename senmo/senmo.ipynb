{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SenMo Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concepts:\n",
    "1. network\n",
    "2. layer\n",
    "3. neuron\n",
    "4. connection\n",
    "\n",
    "example: \n",
    "\n",
    "froward network:\n",
    "layer a predicts b, c, and d, with decreasing accuracy\n",
    "layer b predicts c, and d\n",
    "layer c predicts d\n",
    "layer d predicts nothing\n",
    "\n",
    "backwards network:\n",
    "layer d predicts c, b, and a, with decreasing accuracy\n",
    "layer c predicts b, and a\n",
    "layer b predicts a\n",
    "layer a predicts nothing\n",
    "\n",
    "forwards network competes against backwards network for the highest overall score of predictions. so, layer a in forward network competes against d in backwards network when it predicts d and d predicts a, whoever loses gets harsher errors applied. a also competes against b when they predict each other.\n",
    "\n",
    "predictions are applied in time. a gets activated and produces predictions for all subsequent layers at t=0, but its prediction for b layer is applied right away and checked at t=1, whereas its prediction for c layer is applied at t=1 and checked at t=2, etc.\n",
    "\n",
    "the backwards network is what trains the forward network, they replace back propogation for each other.\n",
    "\n",
    "to be used in a sensorimotor context - an autonomous agent exploring an environment. forward network can be thought of as the sensory network, \"given what I see (layer a), what motor neurons will fire (layer d)?\". backwards network can be seen as the motor network, \"given what motor neurons fire (layer d), what will I see (layer a)?\"\n",
    "\n",
    "this is the simplest form of the idea. for more efficiency shape as an autoencoder. for further efficiencies combine hierarchically.\n",
    "\n",
    "https://www.reddit.com/r/learnmachinelearning/comments/muu060/how_to_instantiate_a_neural_network_design/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "    \n",
    "    def connect(self, neurons: list):\n",
    "        self.neurons = neurons\n",
    "\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.activity = 0\n",
    "        self.active = False\n",
    "\n",
    "    def connect(self, weights: list):\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fire(self):\n",
    "        self.active = True\n",
    "        for weight in self.weights:\n",
    "            weight.activate()\n",
    "\n",
    "\n",
    "class Connection():\n",
    "    def __init__(self, source: Neuron, destination: Neuron, distance: int):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.distance = distance\n",
    "        self.weight = 0\n",
    "        self.activations = []\n",
    "        self.used = []\n",
    "        \n",
    "    def activate(self):\n",
    "        # delay according to distance\n",
    "        self.activations.append(self.weight)\n",
    "    \n",
    "    def deliver(self):\n",
    "        weight = self.activations.pop()\n",
    "        self.destination.activity = self.destination.activity + weight\n",
    "        # wait for it to activate and make sure we re-adjust the weight...\n",
    "        self.used.append(weight)\n",
    "    \n",
    "    def adjust(self):\n",
    "        weight = self.used.pop()\n",
    "        if weight:\n",
    "            error = self.destination.activity - weight\n",
    "            self.weight = weight + (error / 5)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're going to make a 3 layered network:\n",
    "- a  c  e\n",
    "- b  d  f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "first = Layer(0)\n",
    "second = Layer(0)\n",
    "third = Layer(0)\n",
    "a = Neuron(layer=first)\n",
    "b = Neuron(layer=first)\n",
    "c = Neuron(layer=second)\n",
    "d = Neuron(layer=second)\n",
    "e = Neuron(layer=third)\n",
    "f = Neuron(layer=third)\n",
    "ac = Connection(source=a, destination=c, distance=1)\n",
    "ad = Connection(source=a, destination=d, distance=1)\n",
    "ae = Connection(source=a, destination=e, distance=2)\n",
    "af = Connection(source=a, destination=f, distance=2)\n",
    "bc = Connection(source=b, destination=c, distance=1)\n",
    "bd = Connection(source=b, destination=d, distance=1)\n",
    "be = Connection(source=b, destination=e, distance=2)\n",
    "bf = Connection(source=b, destination=f, distance=2)\n",
    "ca = Connection(source=c, destination=a, distance=1)\n",
    "cb = Connection(source=c, destination=b, distance=1)\n",
    "ce = Connection(source=c, destination=e, distance=1)\n",
    "cf = Connection(source=c, destination=f, distance=1)\n",
    "da = Connection(source=d, destination=a, distance=-1)\n",
    "db = Connection(source=d, destination=b, distance=-1)\n",
    "de = Connection(source=d, destination=e, distance=1)\n",
    "df = Connection(source=d, destination=f, distance=1)\n",
    "ea = Connection(source=e, destination=a, distance=-2)\n",
    "eb = Connection(source=e, destination=b, distance=-2)\n",
    "ec = Connection(source=e, destination=c, distance=-1)\n",
    "ed = Connection(source=e, destination=d, distance=-1)\n",
    "fa = Connection(source=f, destination=a, distance=-2)\n",
    "fb = Connection(source=f, destination=b, distance=-2)\n",
    "fc = Connection(source=f, destination=c, distance=-1)\n",
    "fd = Connection(source=f, destination=d, distance=-1)\n",
    "a.connect([ac, ad, ae, af])\n",
    "b.connect([bc, bd, be, bf])\n",
    "c.connect([ca, cb, ce, cf])\n",
    "d.connect([da, db, de, df])\n",
    "e.connect([ea, eb, ec, ed])\n",
    "f.connect([fa, fb, fc, fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "all_weights = [\n",
    "    ac, ad, ae, af, \n",
    "    bc, bd, be, bf, \n",
    "    ca, cb, ce, cf, \n",
    "    da, db, de, df, \n",
    "    ea, eb, ec, ed, \n",
    "    fa, fb, fc, fd]\n",
    "for weight in all_weights:\n",
    "    weight.weight = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize weights to layers\n",
    "sensory_layers = [\n",
    "    [ac, ad, bc, bd],\n",
    "    [ae, af, be, bf],\n",
    "    [ce, cf, de, df]] \n",
    "motor_layers = [\n",
    "    [ca, cb, da, db],\n",
    "    [ea, eb, fa, fb], \n",
    "    [ec, ed, fc, fd]]\n",
    "    \n",
    "for n in [sensory_layers, motor_layers]:\n",
    "    for l in n:\n",
    "        total = sum([c.weight for c in l])\n",
    "        for c in l:\n",
    "            c.weight = c.weight / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, my first design, with all sensory nodes on the left and all motor neurons on the right is correct, but only for the smallest scale. At the larger scale this is effectively false because we don't have all our motor neurons on one side of our head.\n",
    "\n",
    "but we must compress the information flow in both directions at all times. here are my notes:\n",
    "\n",
    "Sensorimotor nodes on the surface of a sphere.\n",
    "\n",
    "Map outputs back to inputs.\n",
    "\n",
    "Move nodes on surface so they are close to each other to minimize bandwidth.\n",
    "\n",
    "When there is surprise, make a hidden node at the best layer inside that describes the fork.\n",
    "\n",
    "...\n",
    "\n",
    "Well intelligence is an union. Every layer gets outside input from deeper within the union and from ultimately, the surface. Each layer takes that input and decides which subset of neurons fire. However, it has, during that round, a new, large set of neurons that are known to fire after this subset, that are put in to a predictive state. Which the outside world, other layers, then choose the subset of those and others too.\n",
    "\n",
    "Predictive means the activation threshold is lowered...\n",
    "\n",
    "There are therefore 3 vectors of prediction, sensory motor, which are conjoined, but signals coming from the deep can be seen basically as coming out to ultimately terminate in motor neurons, while from the surface you get sensory data, and the third is from inside the layer.\n",
    "\n",
    "They condense down the options for each other. We do prediction and compression everywhere in the brain, and we minimize bandwidth.\n",
    "\n",
    "On the smallest scales, there is sensory on the left and motor on the right. When you scale up you realize, not all the network needs to go through the corpus collasum... So you make miniature ones.\n",
    "\n",
    "\"I'm a sensory neuron that is most closely associated with this set of sensory neurons. So if we were arranged on the surface of a sphere I would be in the center of this group. Now perhaps this group is really 2 groups, I fire when two disparate things happen. Then the two groups I'm a part of would kind have me bridging the gap. Etc. Anyway. When I fire I send off a signal to hidden nodes, ones that have learned they fire after me and saying are near by, many are, but don't are far away, many layers in... If they do fire in the next time step then those connections are strengthened, if not they are weakened. That just means we keep track of how many times we activated them and we're correct and how many times we were wrong. It's just an average. How to make these connections highly contextual? Idk. Anyway sometimes I will be put in a predictive state and that means the inner layers think I'll fire next, but I may not, actually, it all depends on the environment since I'm a sensory neuron. If the inner layers predictions are violated the connection weakens but only in that context... Not sure how to manage that... Maybe it'll do it itself.\n",
    "\n",
    "Anyway. Yes those particular connections are weakened which is to say, this combination of neurons is not good at predictions me... But each of those neurons might be good at predicting me in different contexts so... It just doesn't seem fine tuned enough, but maybe hey, it's really not a big deal to black out a small portion... Idk. Anyway. Simply because we are constantly reducing bandwidth, things that are similar get put code together. Semantics are naturally exposed. Encoders are naturally produced. Not sure how, but I can see it leads there.\n",
    "\n",
    "So to recap, there are 2 alternating processes. Inputs from other layers, by the way the further away the connection the higher the threshold for fidelity. You have to be right more than 50% of the time for next layer stuff. But far away connections it's like 90% otherwise the connection is severed. But if course this needs to be converted to significance with a bell curve. New connections are made at random, but these don't last long usually.\n",
    "\n",
    "The other process is quite simply internal to each layer itself. And it's slightly different on the outer most layer than it is on all the hidden layers. On the outer most one the sensory inputs are chosen by the environment. But the layer has the final say on which motor neurons fire. This means and it puts the next set of both in predictive state. If it's right about the next time step of activations then those inter layer connections get stronger. If it's wrong, weaker.\n",
    "\n",
    "So it seems the layer only has the final say half the time. In this view, the external layers send signals, choosing which ones are active, as well as a list of neurons that should be predictive, then the layer chooses which ones are active of those, then chooses a list of prective, that doesn't seem right. \n",
    "\n",
    "Instead the higher layers (deeper) should choose a list of predictives, and the lower layers (outer) should choose which of that list are actually activated. that way from the motor point of view the sensory is higher, and from the sensory point of view the motor is higher. they are each other's boss.\n",
    "\n",
    "what role then is there fore the layer itself? idk, perhaps none. Another thing, no need to have a time delay of predictions, actually. I don't think that's necessary. but I do think it's necessary to have more than 1 layer connections. you need to be able to hear from nodes that are far away. This is looking more and more like a sensorimotor autoencoder. but frankly, perhaps this speaks more to how those autoencoders should be wired up together than it does about anything else. \n",
    "\n",
    "had a realization this morning:\n",
    "Two more things\n",
    "\n",
    "1 each layer has inter connections that go from a node to an inner node. That way specific sets of connections fire based upon the whole layer.\n",
    "\n",
    "2 don't forget distance metric. If a node gets a signal from a node that shares no inputs with the rest of the nodes that this gets a signal from, it's farther away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
